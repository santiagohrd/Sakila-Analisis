# ğŸ§ª ExploraciÃ³n de Datos â€“ Proyecto de Limpieza y ValidaciÃ³n

Este proyecto se enfoca en la **exploraciÃ³n inicial de la base de datos**. El objetivo principal fue identificar inconsistencias, valores nulos, atÃ­picos y validar que los datos cumplan con los formatos esperados antes de pasar a anÃ¡lisis mÃ¡s profundos o modelado.

---

## âœ… Habilidades y conceptos aplicados

### ğŸ” ValidaciÃ³n de tipos de datos

- **Expresiones regulares (`REGEXP`)**: Utilizadas para verificar que los valores de ciertas columnas correspondan al tipo esperado, como nÃºmeros o fechas con formato `YYYY-MM-DD HH:MM:SS`.
- **`STR_TO_DATE()`**: Para validar que las fechas y timestamps sean realmente parseables, evitando errores en columnas que aparentan ser vÃ¡lidas.

### ğŸ§  DetecciÃ³n de valores nulos y anomalÃ­as

- **Uso de `CASE WHEN`**: Para revisar la existencia de valores `NULL` o en blanco en cada columna de forma flexible y personalizada.
- **Descriptivos estadÃ­sticos (media, mediana, desviaciÃ³n estÃ¡ndar, etc.)**: Aplicados a columnas numÃ©ricas para identificar **valores atÃ­picos** (outliers) y posibles errores de ingreso de datos.

### ğŸ—ºï¸ ValidaciÃ³n de coordenadas geogrÃ¡ficas

- AprendÃ­ a usar las funciones espaciales **`ST_X()` y `ST_Y()`** para extraer y validar coordenadas de columnas tipo `GEOMETRY`, asegurando que se encuentren dentro de los rangos geogrÃ¡ficos vÃ¡lidos.

### ğŸ·ï¸ CategorizaciÃ³n de datos

- Uso de la funciÃ³n **`CASE`** para crear nuevas categorÃ­as a partir de valores existentes, o para clasificar registros segÃºn rangos (por ejemplo, edades, rangos salariales, zonas geogrÃ¡ficas).

### ğŸ“† ValidaciÃ³n de fechas

- VerificaciÃ³n de que las fechas estÃ©n **dentro de los rangos esperados**.
- DetecciÃ³n de fechas fuera de rango, errÃ³neas o mal formateadas, usando combinaciones de `STR_TO_DATE`, `BETWEEN`, y `CASE`.

### ğŸ”„ ValidaciÃ³n cruzada entre tablas

- RealicÃ© comparaciones entre columnas relacionadas de diferentes tablas (por ejemplo, `payment_date` vs `rental_date`) para detectar inconsistencias lÃ³gicas como **pagos realizados antes del alquiler**, lo cual revelÃ³ problemas potenciales en la calidad y secuencia de los datos, o posibles agendamientos en la renta.

### ğŸ“Š AnÃ¡lisis temporal de creaciÃ³n de registros

- AgrupÃ© y analicÃ© fechas por **aÃ±o y mes** para entender la distribuciÃ³n temporal de registros, como los clientes (`creation_date`). Esto permitiÃ³ detectar posibles patrones de estacionalidad, errores en fechas o perÃ­odos sin actividad.

### ğŸ’¸ ValidaciÃ³n de reglas de negocio (pagos)

- VerifiquÃ© que los valores de pago (`amount`) fueran vÃ¡lidos, destacando aquellos que eran **cero o negativos**, lo cual puede indicar errores operativos o en la fuente de datos. Esto va mÃ¡s allÃ¡ del tipo de dato y asegura que los valores tengan sentido comercial.

### ğŸ”¢ ValidaciÃ³n de tipos numÃ©ricos en campos de texto

- UtilicÃ© expresiones regulares (`REGEXP`) para confirmar que valores numÃ©ricos almacenados como texto en realidad **cumplen con el formato esperado**, ayudando a identificar entradas incorrectas, como letras o sÃ­mbolos mezclados.

---

## ğŸš§ DesafÃ­os y consideraciones adicionales

Durante este proceso de exploraciÃ³n y preparaciÃ³n de datos, enfrentÃ© varios retos tÃ©cnicos y de lÃ³gica de negocio:

### ğŸ§  Uso de funciones avanzadas en SQL

- El uso de **funciones de ventana**, como `ROW_NUMBER() OVER(PARTITION BY ...)`, representÃ³ un reto inicial, especialmente al momento de detectar duplicados sin eliminar informaciÃ³n valiosa.
- TambiÃ©n trabajÃ© con **funciones de agregaciÃ³n con `OVER()`** para calcular totales acumulados por aÃ±o o categorÃ­a, lo cual implicÃ³ estructurar correctamente la particiÃ³n y el orden.

### ğŸ” ValidaciÃ³n con expresiones regulares

- La validaciÃ³n de columnas con posibles datos incorrectos mediante **`REGEXP`** fue compleja al inicio, pero resultÃ³ esencial para comprobar que campos como montos o correos estuvieran bien formateados.

---

## ğŸ› ï¸ Modificaciones a los datos para enriquecer el anÃ¡lisis

Dado que los datos originales de la base de datos Sakila contenÃ­an valores por defecto o fechas poco realistas (por ejemplo, todas las fechas del 2006), realicÃ© ajustes controlados para simular un conjunto de datos mÃ¡s realista y Ãºtil para anÃ¡lisis:

- ğŸ—“ï¸ **Clientes**: AÃ±adÃ­ una nueva columna `creation_date` con fechas aleatorias entre 2020 y 2025 para representar el momento en que se registraron.
- ğŸ¬ **PelÃ­culas**: AÃ±adÃ­ una columna `released_year` con valores aleatorios entre 2000 y 2020 para simular variedad en los estrenos.
- ğŸŸï¸ **Alquileres (`rental`)**: AjustÃ© la fecha de renta para que estÃ© basada en la fecha de creaciÃ³n del cliente, sumando un intervalo aleatorio.
- ğŸ’° **Pagos (`payment`)**:
  - En la mayorÃ­a de casos, la fecha de pago fue modificada para ocurrir minutos despuÃ©s del alquiler.
  - En el 20% de los registros, el pago fue ajustado para simular una precompra, es decir, ocurriÃ³ **antes** del alquiler.
- ğŸ“¦ **Devoluciones (`return_date`)**: Se generaron fechas de entrega aleatorias entre 1 y 30 dÃ­as despuÃ©s de la fecha de renta.

Estas modificaciones me permitieron aplicar validaciones de calidad temporal mÃ¡s realistas y detectar errores intencionales para fines de aprendizaje.


## ğŸ’¡ ReflexiÃ³n

Este paso me permitiÃ³ entender a fondo la estructura, calidad y desafÃ­os presentes en los datos brutos. La validaciÃ³n con expresiones regulares, el uso de funciones espaciales y la estadÃ­stica descriptiva fueron herramientas clave para garantizar que los datos estÃ©n listos para ser analizados con confianza.








